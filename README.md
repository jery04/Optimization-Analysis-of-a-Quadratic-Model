# Optimization Analysis of a Quadratic Model ğŸ”ğŸ“âœ¨

A small, visual, and hands-on project to study unconstrained optimization of a quadratic function using two classic methods: Steepest Descent with optimal step size and Newtonâ€™s Method. We also generate 3D surfaces and contour plots to visualize the optimization paths. ğŸ¯ğŸ“‰ğŸ—ºï¸

---

## ğŸŒŸ Introduction
This repository explores the minimization of a convex quadratic function of the form

- f(x) = 1/2 xáµ€ Q x + cáµ€ x + r

In this project, the specific scalar function we analyze is:

$$
f(x, y) = (x + y - 7)^2 + (2x + y - 5)^2
$$

We implement and compare:
- Steepest Descent (with analytically optimal step size) ğŸƒâ€â™‚ï¸ğŸ’¨
- Newtonâ€™s Method (one-step solution for quadratics) âš¡ğŸ§ 

Youâ€™ll see trajectories over contour plots and a 3D surface of the objective, making the behavior of both methods crystal clear. ğŸ“ŠğŸŒˆ

---

## ğŸ“ Project Structure

- `index.py` âœ¨
  - All-in-one script that defines the quadratic model, runs Steepest Descent and Newtonâ€™s Method, and renders 3D surface + contour trajectories. Great for a quick demo!
- `Scripts/` ğŸ§©
  - `max_descenso.py` âœ Steepest Descent with optimal step size (Î±â‚– = (gáµ€g)/(gáµ€Qg)); includes a simple CLI demo. ğŸƒâ€â™€ï¸
  - `newton.py` âœ Newtonâ€™s method specialized for quadratics (one iteration to optimal). âš™ï¸
  - `graficar.py` âœ Plotting utilities (3D surface and contour trajectory) and an example that uses `max_descenso.py` and `newton.py`. ğŸ¨ğŸ›°ï¸
- `analisis_optimizacion.ipynb` ğŸ““
  - Optional Jupyter notebook for interactive exploration and visualization.

Artifacts produced when running the demos include printed results and interactive plots. ğŸ–¨ï¸ğŸ“ˆ

---

## ğŸ§  Problem Setup
The example problem used throughout is:

- Q = [[10, 6], [6, 4]]
- c = [-34, -24]
- r = 74

The unique minimizer is x* = -Qâ»Â¹c = (-2, 9), and the minimum value is f(x*) = 0. âœ…

---

## ğŸš€ How to Run
You need Python 3.9+ with NumPy and Matplotlib. From PowerShell on Windows:

```powershell
# 1) Go to the project folder (note the quotes because of spaces/accents)
cd "D:\Modelo de OptimizaciÃ³n"

# 2) (Optional) Activate your environment
# conda activate base

# 3) Quick all-in-one demo: runs methods + shows plots
python index.py
```

You should see console outputs with the optimal point and two plots:
- A 3D surface of f(x, y) ğŸŒ‹
- Contour plots with the optimization paths (Steepest Descent and Newton) ğŸŒ€ğŸ§­

Alternative demo using the modular scripts:

```powershell
cd "D:\Modelo de OptimizaciÃ³n\Scripts"

# Steepest Descent (prints results)
python .\max_descenso.py

# Newtonâ€™s method (prints results)
python .\newton.py

# Plotting demo using both methods
python .\graficar.py
```

---

## âœ… How to Test (Lightweight)
No formal test framework is required. You can verify correctness by checking that:
- Both methods return approximately x* = (-2, 9) ğŸ§²
- The objective at the solution is f(x*) â‰ˆ 0 ğŸŸ¢

Quick interactive check in Python:

```powershell
python - <<'PY'
import numpy as np
from Scripts.max_descenso import maximo_descenso_optimo
from Scripts.newton import newton_quadratic

Q = np.array([[10., 6.],[6., 4.]])
c = np.array([-34., -24.])
r = 74.0
x0 = np.array([0., 0.])

x_sd, info = maximo_descenso_optimo(Q, c, x0, tol=1e-9, max_iter=1000, verbose=False)
x_nt, xs = newton_quadratic(Q, c, x0)

f = lambda x: 0.5 * x @ (Q @ x) + c @ x + r
print("Steepest Descent x*:", x_sd, "f(x*):", f(x_sd))
print("Newton x*:", x_nt, "f(x*):", f(x_nt))
PY
```

Expected output (up to tiny rounding):
- Steepest Descent x* â‰ˆ [-2.  9.], f(x*) â‰ˆ 0.0
- Newton x* â‰ˆ [-2.  9.], f(x*) â‰ˆ 0.0

---

## ğŸ”¬ Methods Overview
- Steepest Descent with optimal step size ğŸƒâ€â™‚ï¸
  - Direction: -âˆ‡f(xâ‚–) = -(Qxâ‚– + c)
  - Step: Î±â‚– = (gáµ€g)/(gáµ€Qg)
  - Converges linearly on quadratics; the optimal Î±â‚– formula ensures efficient progress.

- Newtonâ€™s Method ğŸ§ 
  - For quadratics, one iteration from any xâ‚€ yields x* = -Qâ»Â¹c.
  - In practice, we solve Qp = -(Qxâ‚€ + c) and take xâ‚ = xâ‚€ + p.

---

## ğŸ§© Visualizations
- 3D surface of f(x, y) with a color map ğŸŒˆ
- Contour plots with overlayed trajectories ğŸ”
- Customizable ranges, resolution, markers, and levels ğŸ›ï¸

These make it easy to compare the behavior of both methods and see the path to the minimum. ğŸ‘€ğŸ“

---

## ğŸ§¾ Conclusions
- Newtonâ€™s method hits the exact minimizer in a single iteration for quadratic objectives. âš¡
- Steepest Descent with the optimal step size converges reliably, illustrating a zig-zag path when level sets are elongated. ğŸ”»â¡ï¸ğŸ”»
- Visualizations confirm both methods reach x* = (-2, 9) with f(x*) = 0. ğŸ‰
- The project serves as a compact, didactic reference for quadratic optimization and method comparison. ğŸ“š

---

## ğŸ“¦ Requirements
- Python 3.9+
- NumPy ğŸ§®
- Matplotlib ğŸ“Š

Install (optional, if needed):
```powershell
pip install numpy matplotlib
```

---

## ğŸ“ Notes
- Matrices are symmetrized defensively in Steepest Descent to avoid numerical issues. ğŸ›¡ï¸
- The plotting functions include sensible defaults and options for camera/view control. ğŸ¥
- Paths with spaces/accents require quoting in Windows PowerShell (as shown). ğŸªŸ

Enjoy exploring optimization with math and visuals! ğŸ’«